[drvq](http://image.ntua.gr/iva/tools/drvq)
====

*dimensionality-recursive vector quantization*

`drvq` is a C++ library implementation of [dimensionality-recursive vector quantization](http://image.ntua.gr/iva/research/drvq), a fast vector quantization method in high-dimensional Euclidean spaces under arbitrary data distributions.

Licence
-------

`drvq` has a 2-clause BSD license. See file LICENSE for the complete license text.

Directory structure
-------------------

`drvq` library constists primarily of template C++ header-only (`.hpp`) code, but also includes a number of command-line tools to test the library, provided as `.cpp` files. The directory structure is:

	/data      sample data files for testing
	/matlab    binary file input/output tools for matlab
	/out       output folder for drvq tools
	/run       simple scripts to compile and execute
	/src       drvq library and tools source code

Requirements
------------

`drvq` implementation is based on C++ template library [ivl](http://image.ntua.gr/ivl/), which may be downloaded from [sourceforge.net](http://sourceforge.net/projects/ivl/files/) and [installed](http://image.ntua.gr/ivl/download.php) on a number of platforms.

Installation, building
----------------------

No installation is needed or provided. Command-line tools under folder [/src/](/src/) illustrate how the library may be used and may be compiled with minimal options without a Makefile, and scripts under [/run/](/run/) show how a tool may be compiled and executed, using either Clang or GCC.

`drvq` has only been tested on [clang 3.3](http://llvm.org/releases/download.html#3.3) and [g++ 4.8.1](http://gcc.gnu.org/gcc-4.8/) on Linux, but it should be straightfoward to use on other platforms.

Script [/run/run](/run/run) is for GCC and [/run/lrun](/run/lrun) for Clang. It is best to copy them in a folder in your path, e.g. `/usr/local/bin`, after adjusting folders for your local copy of `ivl`. In this case, starting from the folder where `drvq` resides,

	cd src/
	run train

compiles file [/src/train.cpp](/src/train.cpp), produces binary `/src/train`, and executes it unless there are compiler or linker errors. If `ivl` is properly installed, compilation may be even simpler, without specifying the include folder with `-I`.

Extension `.cpp` is not necessary for the main source file. Additional source files or compiler options may be specified as additional command-line arguments, but extensions are needed for extra source files.

Binary file format
------------------

All input/output to all `drvq` tools is based on a binary file format, using a file extension `.bin`. With the exception of codebooks (file `codebook.bin`) produced by tool `train` and used by other tools, all other input/output files each contain an array of arrays of numbers of any type.

Such files may be read/written by Matlab scripts [load_double_array.m](/matlab/load_double_array.m) / [save_double_array.m](/matlab/save_double_array.m) under folder [/matlab/](/matlab/). An array of arrays is is represented by a two-dimensional matrix in Matlab, but any Matlab data type may be used. These scripts are very simple and may serve as a specification for input/output tools in other platforms. For C++, input/output is handled by template functions under [/src/lib](/src/lib).

Sample data
-----------

With the specific information provided per tool below, one should be able to generate appropriate input and read the output of each tool. For convenience, a number of sample data files are provided under folder [/data/](/data/) so that all `drvq` tools may run immediately.

These files are part of the actual experimental data that were used during development. In particular, under [/data/oxford/hesaff.sift/queries/descriptors/](/data/oxford/hesaff.sift/queries/descriptors/), SIFT descriptors are given for each of the 55 cropped query images of [Oxford buildings dataset](http://www.robots.ox.ac.uk/~vgg/data/oxbuildings/), as obtained by [Hessian-affine detector](https://github.com/perdoch/hesaff). File [/data/oxford/files/queries.txt](/data/oxford/files/queries.txt) contains a relevant list of filenames.

For each image, a binary file contains an array of descriptors, where each descriptor is represented by an array of `float`s. A file of the same format can be easily generated by [load_double_array.m](/matlab/load_double_array.m) given a two-dimensional matrix of `float`s in Matlab, where rows (columns) correspond to local features (descriptor elements/dimensions).

Tools
-----

All tools are provided as very simple, script-like `.cpp` files of few lines of code each, with all low-level implementation hidden in corresponding `.hpp` or other included files. Each `.cpp` file has a `main()` function and generates a command-line executable that can be used as a stand-alone tool for a particular job, using a rich set of command-line options. On the other hand, its source code illustrates how the library can be used, e.g. to integrate in other programs.

The usage of each tool can be seen by option `-h` or `--help`, including a short description of each command-line option. Some additional documentation is provided below. In all cases, hard-coded defaults enable a "demo" operation on the sample data with no options given at all.

### `train`

Specified by [train.cpp](/src/train.cpp). Reads a set of input training data like those provided under [/data/](/data/), representing a set of points (vectors) in a Euclidean space, trains one or more codebooks, and saves the codebooks in a custom binary format that is to be read by other tools. The codebooks are internally structured as binary trees over the dimensions of the space and contain additional data like lookup tables to be used for fast encoding (quantization) of new data.

The first few options refer to input/output files, paths, etc. and are rather self-explanatory. Option `--dataset` acts like a preset for other such options. All "presets" are hard-coded in [/src/options/data.hpp](/src/options/data.hpp) are can be freely changed in the source code, but each option can also be overriden. Option `--unit` is only used when a large dataset is split into individual units (chunks) stored under different folders.

Because all experiments carried out refer to descriptors of images, all data are assumed to reside in individual files, one for each image. In a different application, all data may reside in a single file; in this case, the input file list provided by option `--list` would only contain a single filename.

For the same, although the method supports arbitrary dimensions, only two alternatives are provided here: 64 dimensions for SURF descriptors, or 128 dimensions for SIFT descriptors, controlled by `--descriptor`. Each vector may be used as a whole, producing a single codebook, or split into sub-vectors, producing more codebooks. This is controlled by `--books`, supporting 1, 2, or 4 codebooks. E.g. SIFT vectors of 128 dimensions and 4 codebooks result in 4 sub-vectors of 32 dimensions each.

Because the method is hierarchical in the number of dimensions, there is no single parameter for the target codebook size (number of centroids), but rather one such target for each level of the hierarchy. This is controlled by "preset" codebook sizes (capacities) specified in [/src/options/train.hpp](/src/options/train.hpp) and controlled by option `--capacity`. E.g. the default setting `2` for SIFT and 4 codebooks corresponds to entry

	5, 6, 7, 8, 9, 11

which represents the codebook size per number of dimensions, with both expressed as powers of two. E.g. `2^5 = 32` centroids for `2^0 = 1` dimension, `2^6 = 64` centroids for `2^1 = 2` dimensions and so on; finally, `2^11 = 2048` centroids for `2^6 = 32` dimensions. One may freely manipulate these presets for a different application, but codebook size should generally increase with dimension, and should not increase too much beyond `2^12` or training will take too much space and time.

Termination of training takes into account both progress towards convergence and the actual number of iterations so far. It is controlled by a single parameter `--theta`. The value should be positive; a lower value results in longer training.

### `flat`

### `nn`

### `label`

Citation
--------

Please cite the following paper if you use this software.

Yannis Avrithis. [Quantize and Conquer: A dimensionality-recursive solution to clustering, vector quantization, and image retrieval](http://image.ntua.gr/iva/publications/qc). In Proceedings of International Conference on Computer Vision (ICCV 2013), December 2013.
